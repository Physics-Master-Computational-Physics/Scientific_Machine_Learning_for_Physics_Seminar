{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE) from Scratch in PyTorch\n",
    "\n",
    "This notebook demonstrates how to build a Variational Autoencoder (VAE) for the MNIST dataset using PyTorch. We will implement the core components by hand and explain the connection between the code and the underlying theory and equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to VAEs\n",
    "\n",
    "Variational Autoencoders (VAEs) are generative models that learn a probability distribution over the input data. They consist of two main parts:\n",
    "\n",
    "1.  **Encoder (Recognition Model):** $q_\\phi(z|x)$. This network takes an input $x$ and outputs parameters (typically mean $\\mu$ and log-variance $\\log \\sigma^2$) for a probability distribution in the latent space, $z$. This distribution approximates the true posterior $p(z|x)$.\n",
    "2.  **Decoder (Generative Model):** $p_\\theta(x|z)$. This network takes a sample $z$ from the latent distribution and reconstructs the input data $x$.\n",
    "\n",
    "The goal of a VAE is to maximize the **Evidence Lower Bound (ELBO)**, which is a lower bound on the log-likelihood of the data $\\log p(x)$:\n",
    "\n",
    "$$ \\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z)) $$ \n",
    "\n",
    "Where:\n",
    "- The first term, $\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]$, is the **reconstruction loss**. It encourages the decoder to accurately reconstruct the input $x$ from the latent representation $z$.\n",
    "- The second term, $D_{KL}(q_\\phi(z|x) || p(z))$, is the **Kullback-Leibler (KL) divergence** between the approximate posterior $q_\\phi(z|x)$ learned by the encoder and a prior distribution $p(z)$ over the latent variables (usually a standard normal distribution, $\\mathcal{N}(0, I)$). This term acts as a regularizer, forcing the latent space to be well-behaved (e.g., continuous and centered around the origin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameters and Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20 # Increase for better results, e.g., 20-50\n",
    "LEARNING_RATE = 1e-3\n",
    "INPUT_DIM = 28 * 28  # MNIST image dimensions (flattened)\n",
    "HIDDEN_DIM = 400    # Dimension of hidden layers\n",
    "LATENT_DIM = 20     # Dimension of the latent space z\n",
    "\n",
    "# For visualizing 2D latent space (optional, set LATENT_DIM=2 for this)\n",
    "# LATENT_DIM = 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Converts to [0, 1] tensor\n",
    "    # transforms.Normalize((0.1307,), (0.3081,)) # Optional: Standard MNIST normalization\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. VAE Model Definition\n",
    "\n",
    "We'll define the VAE as a `nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Encoder Network: $q_\\phi(z|x)$\n",
    "\n",
    "The encoder network takes an input image $x$ and maps it to the parameters of a Gaussian distribution in the latent space. Specifically, it outputs a mean vector $\\mu_z$ and a log-variance vector $\\log(\\sigma_z^2)$. We use log-variance for numerical stability and to ensure $\\sigma_z^2$ is always positive.\n",
    "\n",
    "Structure:\n",
    "1.  Input layer (flattened image)\n",
    "2.  One or more hidden layers (e.g., with ReLU activation)\n",
    "3.  Two separate linear output layers: one for $\\mu_z$ and one for $\\log(\\sigma_z^2)$.\n",
    "\n",
    "The output of the encoder defines $q_\\phi(z|x) = \\mathcal{N}(z; \\mu_z, \\text{diag}(\\sigma_z^2))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Reparameterization Trick\n",
    "\n",
    "To train the VAE using backpropagation, we need to sample $z$ from $q_\\phi(z|x)$. However, the sampling operation itself is stochastic and non-differentiable. The **reparameterization trick** allows us to re-express the sampling process in a way that makes it differentiable.\n",
    "\n",
    "Instead of sampling $z \\sim \\mathcal{N}(\\mu_z, \\sigma_z^2)$, we sample a noise variable $\\epsilon \\sim \\mathcal{N}(0, I)$ (a standard normal distribution, which is parameter-free) and then compute $z$ as:\n",
    "\n",
    "$$ z = \\mu_z + \\sigma_z \\odot \\epsilon $$\n",
    "\n",
    "Here, $\\odot$ denotes element-wise multiplication. Since $\\sigma_z = \\exp(0.5 \\cdot \\log(\\sigma_z^2))$, the equation becomes:\n",
    "\n",
    "$$ z = \\mu_z + \\exp(0.5 \\cdot \\log(\\sigma_z^2)) \\odot \\epsilon $$\n",
    "\n",
    "Now, the stochasticity comes from $\\epsilon$, which is an input to this computation. The transformation from $\\epsilon$, $\\mu_z$, and $\\log(\\sigma_z^2)$ to $z$ is deterministic, allowing gradients to flow back through $\\mu_z$ and $\\log(\\sigma_z^2)$ to the encoder parameters $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Decoder Network: $p_\\theta(x|z)$\n",
    "\n",
    "The decoder network takes a latent sample $z$ and maps it back to the data space, aiming to reconstruct the original input $x$. Its output, $\\hat{x}$, represents the parameters of the distribution $p_\\theta(x|z)$. For MNIST, where pixel values are typically between 0 and 1, we often model $p_\\theta(x|z)$ as a Bernoulli distribution for each pixel. The decoder outputs the probability parameter for each pixel, often using a sigmoid activation function.\n",
    "\n",
    "Structure:\n",
    "1.  Input layer (latent vector $z$)\n",
    "2.  One or more hidden layers (e.g., with ReLU activation)\n",
    "3.  Output layer (reconstructed image, with sigmoid activation for pixel values in [0,1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # === Encoder Part ===\n",
    "        # q_phi(z|x)\n",
    "        # This part maps input x to an intermediate hidden representation\n",
    "        self.fc_encoder_hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        # From the hidden representation, we get mu and log_var\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)      # To output mu\n",
    "        self.fc_log_var = nn.Linear(hidden_dim, latent_dim) # To output log_var (log(sigma^2))\n",
    "\n",
    "        # === Decoder Part ===\n",
    "        # p_theta(x|z)\n",
    "        # This part maps latent z back to an intermediate hidden representation\n",
    "        self.fc_decoder_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        # From the hidden representation, we reconstruct x\n",
    "        self.fc_output = nn.Linear(hidden_dim, input_dim) # To output reconstructed x\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Maps input x to mu and log_var for the latent space distribution q_phi(z|x).\"\"\"\n",
    "        # x -> hidden_dim\n",
    "        h_encoder = F.relu(self.fc_encoder_hidden(x))\n",
    "        # hidden_dim -> latent_dim (for mu and log_var)\n",
    "        mu = self.fc_mu(h_encoder)\n",
    "        log_var = self.fc_log_var(h_encoder)\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Implements the reparameterization trick: z = mu + sigma * epsilon.\n",
    "        epsilon is sampled from N(0, I).\n",
    "        sigma is calculated from log_var: sigma = exp(0.5 * log_var).\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var) # sigma = sqrt(exp(log_var)) = exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)    # Sample epsilon from N(0, I) with same shape as std\n",
    "        return mu + eps * std          # z = mu + epsilon * sigma\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Maps latent vector z back to the reconstructed data space x_hat.\"\"\"\n",
    "        # z -> hidden_dim\n",
    "        h_decoder = F.relu(self.fc_decoder_hidden(z))\n",
    "        # hidden_dim -> input_dim (reconstructed x)\n",
    "        # We use sigmoid because MNIST pixels are in [0, 1].\n",
    "        # This output represents the parameters of a Bernoulli distribution for each pixel.\n",
    "        recon_x = torch.sigmoid(self.fc_output(h_decoder)) \n",
    "        return recon_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass: x -> mu, log_var -> z -> recon_x.\"\"\"\n",
    "        # Flatten input for MLP\n",
    "        x_flat = x.view(-1, self.input_dim) \n",
    "        \n",
    "        # 1. Encode x to get parameters of q_phi(z|x)\n",
    "        mu, log_var = self.encode(x_flat)\n",
    "        \n",
    "        # 2. Sample z from q_phi(z|x) using reparameterization trick\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # 3. Decode z to get parameters of p_theta(x|z) (reconstructed x)\n",
    "        recon_x = self.decode(z)\n",
    "        \n",
    "        return recon_x, mu, log_var\n",
    "\n",
    "# Instantiate the model\n",
    "model = VAE(INPUT_DIM, HIDDEN_DIM, LATENT_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Function (ELBO)\n",
    "\n",
    "As mentioned, we want to maximize the ELBO:\n",
    "$$ \\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z)) $$\n",
    "\n",
    "In practice, we minimize the negative ELBO:\n",
    "$$ -\\mathcal{L} = -\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] + D_{KL}(q_\\phi(z|x) || p(z)) $$\n",
    "\n",
    "This is often written as:\n",
    "$$ \\text{Loss} = \\text{Reconstruction Loss} + \\text{KL Divergence} $$\n",
    "\n",
    "1.  **Reconstruction Loss**: For Bernoulli distributed outputs (pixel values in [0,1] from sigmoid), the term $-\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]$ becomes the **Binary Cross-Entropy (BCE)** between the original input $x$ and the reconstructed output $\\hat{x}$ (which are the Bernoulli parameters from the decoder).\n",
    "    $$ \\text{BCE}(x, \\hat{x}) = - \\sum_{i=1}^{\\text{dim}(x)} [x_i \\log(\\hat{x}_i) + (1-x_i) \\log(1-\\hat{x}_i)] $$\n",
    "    PyTorch's `F.binary_cross_entropy` with `reduction='sum'` calculates this.\n",
    "\n",
    "2.  **KL Divergence**: The KL divergence between $q_\\phi(z|x) = \\mathcal{N}(\\mu_z, \\text{diag}(\\sigma_z^2))$ and the prior $p(z) = \\mathcal{N}(0, I)$ has an analytical solution:\n",
    "    $$ D_{KL}(\\mathcal{N}(\\mu_z, \\sigma_z^2) || \\mathcal{N}(0, I)) = -0.5 \\sum_{j=1}^{\\text{LATENT_DIM}} (1 + \\log(\\sigma_{z,j}^2) - \\mu_{z,j}^2 - \\sigma_{z,j}^2) $$\n",
    "    Where $j$ indexes the dimensions of the latent space. Remember $\\sigma_{z,j}^2 = \\exp(\\log(\\sigma_{z,j}^2))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    \"\"\"\n",
    "    Computes the VAE loss, which is the sum of reconstruction loss and KL divergence.\n",
    "    recon_x: a tensor of shape (batch_size, input_dim) - reconstructed input\n",
    "    x: a tensor of shape (batch_size, input_dim) - original input\n",
    "    mu: a tensor of shape (batch_size, latent_dim) - latent mean\n",
    "    log_var: a tensor of shape (batch_size, latent_dim) - latent log variance\n",
    "    \"\"\"\n",
    "    # 1. Reconstruction Loss (Binary Cross Entropy)\n",
    "    # We use F.binary_cross_entropy. It expects recon_x (probabilities) and x (binary targets).\n",
    "    # The sum over dimensions is important for proper weighting against KLD.\n",
    "    # x.view(-1, INPUT_DIM) ensures x is flattened just like recon_x\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, INPUT_DIM), reduction='sum')\n",
    "    \n",
    "    # 2. KL Divergence\n",
    "    # D_KL(q_phi(z|x) || p(z)) = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # where sigma^2 = exp(log_var)\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    \n",
    "    # Total loss\n",
    "    return BCE + KLD\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train() # Set model to training mode\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader): # We don't need labels for VAE\n",
    "        data = data.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        recon_batch, mu, log_var = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ' \n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(data):.6f}')\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_train_loss:.4f}')\n",
    "    return avg_train_loss\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad(): # No gradients needed for evaluation\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(DEVICE)\n",
    "            recon_batch, mu, log_var = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, log_var).item()\n",
    "            \n",
    "            # For the first batch in the first epoch, save original and reconstructed images for visualization\n",
    "            if i == 0 and epoch % 5 == 0: # Show every 5 epochs\n",
    "                n = min(data.size(0), 8) # Display up to 8 images\n",
    "                comparison = torch.cat([data[:n].view(-1, 1, 28, 28), # Original\n",
    "                                      recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]]) # Reconstructed\n",
    "                # Convert to numpy for plotting\n",
    "                comparison_np = comparison.cpu().numpy().squeeze()\n",
    "                \n",
    "                fig, axes = plt.subplots(2, n, figsize=(n*1.5, 3))\n",
    "                for img_idx in range(n):\n",
    "                    axes[0, img_idx].imshow(comparison_np[img_idx], cmap='gray')\n",
    "                    axes[0, img_idx].set_title(\"Original\")\n",
    "                    axes[0, img_idx].axis('off')\n",
    "                    axes[1, img_idx].imshow(comparison_np[n+img_idx], cmap='gray')\n",
    "                    axes[1, img_idx].set_title(\"Recon\")\n",
    "                    axes[1, img_idx].axis('off')\n",
    "                plt.suptitle(f\"Epoch {epoch} Reconstructions\", fontsize=16)\n",
    "                plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "                plt.show()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    print(f'====> Test set loss: {avg_test_loss:.4f}')\n",
    "    return avg_test_loss\n",
    "\n",
    "# Start training\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(epoch)\n",
    "    test_loss = test(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('VAE Training and Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Generating New Samples\n",
    "\n",
    "To generate new samples, we sample $z$ from the prior distribution $p(z) = \\mathcal{N}(0, I)$ and pass it through the decoder $p_\\theta(x|z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(num_samples=64):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample z from the prior N(0, I)\n",
    "        z_sample = torch.randn(num_samples, LATENT_DIM).to(DEVICE)\n",
    "        # Decode z to generate new images\n",
    "        generated_images = model.decode(z_sample).cpu()\n",
    "        \n",
    "    # Reshape and plot\n",
    "    generated_images = generated_images.view(num_samples, 1, 28, 28)\n",
    "    \n",
    "    grid_size = int(np.sqrt(num_samples))\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size*1.2, grid_size*1.2))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_samples:\n",
    "            ax.imshow(generated_images[i].squeeze(), cmap='gray')\n",
    "            ax.axis('off')\n",
    "    plt.suptitle('Generated Samples from Latent Space', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "generate_samples(num_samples=16) # Generate 16 samples (4x4 grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Visualizing Latent Space (if LATENT_DIM = 2)\n",
    "\n",
    "If `LATENT_DIM` was set to 2, we can visualize how the MNIST digits are distributed in this 2D latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space():\n",
    "    if LATENT_DIM != 2:\n",
    "        print(\"Latent space visualization is only for LATENT_DIM=2.\")\n",
    "        print(f\"Current LATENT_DIM is {LATENT_DIM}.\")\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    latent_vectors = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader: # Use test_loader for a manageable number of points\n",
    "            data = data.to(DEVICE)\n",
    "            mu, _ = model.encode(data.view(-1, INPUT_DIM))\n",
    "            latent_vectors.append(mu.cpu())\n",
    "            labels_list.append(labels.cpu())\n",
    "    \n",
    "    latent_vectors = torch.cat(latent_vectors).numpy()\n",
    "    labels_list = torch.cat(labels_list).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    scatter = plt.scatter(latent_vectors[:, 0], latent_vectors[:, 1], c=labels_list, cmap='tab10', alpha=0.7, s=10)\n",
    "    plt.colorbar(scatter, ticks=range(10))\n",
    "    plt.title('MNIST Latent Space ($\\mu$ vectors)')\n",
    "    plt.xlabel('Latent Dimension 1')\n",
    "    plt.ylabel('Latent Dimension 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "This notebook provided a step-by-step implementation of a Variational Autoencoder in PyTorch.\n",
    "\n",
    "Key takeaways:\n",
    "- **Encoder $q_\\phi(z|x)$**: Maps input $x$ to parameters $(\\mu, \\log \\sigma^2)$ of a Gaussian latent distribution.\n",
    "- **Reparameterization Trick**: $z = \\mu + \\sigma \\odot \\epsilon$, enabling backpropagation through the sampling process.\n",
    "- **Decoder $p_\\theta(x|z)$**: Maps latent sample $z$ back to reconstruct $x$.\n",
    "- **Loss Function (ELBO)**: Combines reconstruction loss (e.g., BCE) and KL divergence to regularize the latent space.\n",
    "\n",
    "VAEs are powerful generative models capable of learning complex data distributions and generating new, realistic samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}